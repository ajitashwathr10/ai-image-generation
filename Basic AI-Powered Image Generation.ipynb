{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'load_img' from 'tensorflow.keras.preprocessing' (C:\\Users\\AJIT ASHWATH R\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\keras\\preprocessing\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPProcessor, CLIPModel, CLIPVisionModel\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inception_v3\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_img, img_to_array\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_img' from 'tensorflow.keras.preprocessing' (C:\\Users\\AJIT ASHWATH R\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\keras\\preprocessing\\__init__.py)"
          ]
        }
      ],
      "source": [
        "#Import modules\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "from tensorflow.keras.preprocessing import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_models(clip_model = \"openai/clip-vit-base-patch32\", inception_weights = \"imagenet\"):\n",
        "    processor = CLIPProcessor.from_pretrained(clip_model)\n",
        "    clip_model = CLIPModel.from_pretrained(clip_model).to(DEVICE)\n",
        "    inception_model = inception_v3.InceptionV(weights = inception_weights, include_top = False)\n",
        "    return processor, clip_model, inception_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_image(img_path, target_size = (300, 300)):\n",
        "    img = load_img(img_path, target_size = target_size)\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).float()\n",
        "    return img_tensor.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_noise(shape):\n",
        "    return torch.randn(*shape, device = DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_virtual_image(\n",
        "        base_img_path,\n",
        "        text_prompt,\n",
        "        processor,\n",
        "        clip_model,\n",
        "        iterations = 20,\n",
        "        learnig_rate = 0.1,\n",
        "):\n",
        "    base_img_path = preprocess_image(base_img_path)\n",
        "    virtual_img = generate_noise(base_img_path.shape).requires_grad_(True)\n",
        "    text_inputs = processor(text = text_prompt, return_tensors = \"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.get_text_features(**text_inputs)\n",
        "    for _ in range(iterations):\n",
        "        image_features = clip_model.get_image_features(virtual_img)\n",
        "        similarity_loss = -F.cosine.similarity(text_features, image_features).mean()\n",
        "        similarity_loss.backward()\n",
        "        virtual_img.data -= learnig_rate * virtual_img.grad\n",
        "        virtual_img.grad.zero_()\n",
        "    virtual_img_np = virtual_img.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "    virtual_img_np = (virtual_img_np * 255).astype(np.uint8)\n",
        "\n",
        "    base_img = cv2.imread(base_img_path)\n",
        "    virtual_img_resized = cv2.resize(virtual_img_np, (base_img.shape[1], base_img.shape[0]))\n",
        "    blended_img = cv2.addWeighted(base_img, 0.5, virtual_img_resized, 0.5, 0)\n",
        "\n",
        "    return blended_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    processor, clip_model, _ = load_models()\n",
        "    base_img_path = input(\"Enter base image path:\")\n",
        "    text_prompt = input(\"Enter text description for image generation:\")\n",
        "    generated_image = generate_virtual_image(\n",
        "        base_img_path,\n",
        "        text_prompt,\n",
        "        processor,\n",
        "        clip_model\n",
        "    )\n",
        "    cv2.imwrite(\"generated_image.jpg\", generated_image)\n",
        "    print(\"Virtual image generated and saved as 'generated_image.jpg'\")\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
